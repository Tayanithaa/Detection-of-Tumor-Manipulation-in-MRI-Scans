{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f23ff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b0dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (7.2.0)\n",
      "Requirement already satisfied: jupyter-client>=8.8.0 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipykernel) (8.8.0)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipykernel) (27.1.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipykernel) (1.8.20)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipykernel) (8.38.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipykernel) (0.2.1)\n",
      "Requirement already satisfied: tornado>=6.4.1 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipykernel) (6.5.4)\n",
      "Requirement already satisfied: packaging>=22 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipykernel) (26.0)\n",
      "Requirement already satisfied: jupyter-core!=6.0.*,>=5.1 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipykernel) (5.9.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipykernel) (7.2.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipykernel) (0.2.3)\n",
      "Requirement already satisfied: stack_data in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: decorator in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: colorama in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Requirement already satisfied: exceptiongroup in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (1.3.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (4.15.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from jupyter-client>=8.8.0->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from jupyter-core!=6.0.*,>=5.1->ipykernel) (4.9.2)\n",
      "Requirement already satisfied: wcwidth in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.8.0->ipykernel) (1.17.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\projects\\deepfake_tumor\\detection-of-tumor-manipulation-in-mri-scans\\venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 26.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Projects\\DeepFake_Tumor\\Detection-of-Tumor-Manipulation-in-MRI-Scans\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ipykernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f4a1ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (408055947.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -m ipykernel install --user --name=deepfake_venv --display-name \"Python (Deepfake Venv)\"\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "python -m ipykernel install --user --name=deepfake_venv --display-name \"Python (Deepfake Venv)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e834fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\DeepFake_Tumor\\Detection-of-Tumor-Manipulation-in-MRI-Scans\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "532c3dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\DeepFake_Tumor\\Detection-of-Tumor-Manipulation-in-MRI-Scans\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ebf5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_PATH = \"Dataset/Raw\"\n",
    "PROCESSED_PATH = \"Dataset/Processed\"\n",
    "\n",
    "tumor_classes = [\"glioma\", \"meningioma\", \"pituitary\"]\n",
    "non_tumor_class = \"notumor\"\n",
    "\n",
    "\n",
    "def extract_center_patch(img):\n",
    "    h, w = img.shape[:2]\n",
    "    size = 80   # fixed size for stability\n",
    "\n",
    "    cx, cy = w // 2, h // 2\n",
    "    x1 = cx - size // 2\n",
    "    y1 = cy - size // 2\n",
    "\n",
    "    patch = img[y1:y1+size, x1:x1+size]\n",
    "    mask = 255 * np.ones(patch.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    return patch, mask\n",
    "\n",
    "\n",
    "def generate_deepfake(split):\n",
    "    print(f\"\\nGenerating {split} set...\")\n",
    "\n",
    "    raw_split = os.path.join(RAW_PATH, split)\n",
    "    processed_split = os.path.join(PROCESSED_PATH, split.lower())\n",
    "\n",
    "    real_dir = os.path.join(processed_split, \"real\")\n",
    "    fake_dir = os.path.join(processed_split, \"manipulated\")\n",
    "\n",
    "    os.makedirs(real_dir, exist_ok=True)\n",
    "    os.makedirs(fake_dir, exist_ok=True)\n",
    "\n",
    "    tumor_images = []\n",
    "    for cls in tumor_classes:\n",
    "        cls_path = os.path.join(raw_split, cls)\n",
    "        for f in os.listdir(cls_path):\n",
    "            tumor_images.append(os.path.join(cls_path, f))\n",
    "\n",
    "    non_tumor_path = os.path.join(raw_split, non_tumor_class)\n",
    "    non_tumor_files = os.listdir(non_tumor_path)\n",
    "\n",
    "    for i in tqdm(range(len(non_tumor_files))):\n",
    "        healthy_img = cv2.imread(os.path.join(non_tumor_path, non_tumor_files[i]))\n",
    "        tumor_img = cv2.imread(random.choice(tumor_images))\n",
    "\n",
    "        if healthy_img is None or tumor_img is None:\n",
    "            continue\n",
    "\n",
    "        healthy_img = cv2.resize(healthy_img, (224, 224))\n",
    "        tumor_img = cv2.resize(tumor_img, (224, 224))\n",
    "\n",
    "        patch, mask = extract_center_patch(tumor_img)\n",
    "\n",
    "        center = (random.randint(60, 160), random.randint(60, 160))\n",
    "\n",
    "        deepfake = cv2.seamlessClone(patch, healthy_img, mask, center, cv2.NORMAL_CLONE)\n",
    "\n",
    "        cv2.imwrite(os.path.join(real_dir, f\"real_{i}.jpg\"), healthy_img)\n",
    "        cv2.imwrite(os.path.join(fake_dir, f\"fake_{i}.jpg\"), deepfake)\n",
    "\n",
    "    print(f\"{split} generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6f323d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:41<00:00, 33.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training generation complete.\n",
      "\n",
      "Generating Testing set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:10<00:00, 37.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing generation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_deepfake(\"Training\")\n",
    "generate_deepfake(\"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b338a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manipulated', 'real']\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "train_dir = \"Dataset/Processed/training\"\n",
    "test_dir = \"Dataset/Processed/testing\"\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "print(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9408e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5678735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0eee809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf3664ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20854a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['manipulated', 'real']\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"Dataset/Processed/training\"\n",
    "test_dir = \"Dataset/Processed/testing\"\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Classes:\", train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdeadf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PHASE 1: Training classifier head\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Freeze backbone\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final layer\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "print(\"\\nPHASE 1: Training classifier head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f588237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:17<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5751596368210656\n",
      "Train Accuracy: 69.0\n",
      "\n",
      "Epoch [2/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:12<00:00, 26.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5187545452799116\n",
      "Train Accuracy: 74.60714285714286\n",
      "\n",
      "Epoch [3/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:12<00:00, 29.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.45688428344471116\n",
      "Train Accuracy: 78.0\n",
      "\n",
      "Epoch [4/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:12<00:00, 28.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4547274281510285\n",
      "Train Accuracy: 78.64285714285714\n",
      "\n",
      "Epoch [5/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:12<00:00, 27.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4329019460082054\n",
      "Train Accuracy: 80.78571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch+1}/5]\")\n",
    "\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\"Loss:\", running_loss / len(train_loader))\n",
    "    print(\"Train Accuracy:\", 100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4cac384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PHASE 2: Fine-tuning full model\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze everything\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "print(\"\\nPHASE 2: Fine-tuning full model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c389926e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:21<00:00, 16.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.30468023392371835\n",
      "Train Accuracy: 88.5\n",
      "\n",
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:20<00:00, 17.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1285106645484588\n",
      "Train Accuracy: 95.67857142857143\n",
      "\n",
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:21<00:00, 16.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.10144435184026536\n",
      "Train Accuracy: 96.92857142857143\n",
      "\n",
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:22<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06872560511135296\n",
      "Train Accuracy: 97.96428571428571\n",
      "\n",
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:21<00:00, 16.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06254254167067952\n",
      "Train Accuracy: 98.03571428571429\n",
      "\n",
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:21<00:00, 16.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.057300514051624174\n",
      "Train Accuracy: 98.35714285714286\n",
      "\n",
      "Epoch [7/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:21<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.044940400819759814\n",
      "Train Accuracy: 98.57142857142857\n",
      "\n",
      "Epoch [8/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:21<00:00, 16.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.03577286473974319\n",
      "Train Accuracy: 98.78571428571429\n",
      "\n",
      "Epoch [9/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:21<00:00, 16.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0412394151821666\n",
      "Train Accuracy: 98.53571428571429\n",
      "\n",
      "Epoch [10/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:21<00:00, 16.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.03290750591691384\n",
      "Train Accuracy: 99.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch+1}/10]\")\n",
    "\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\"Loss:\", running_loss / len(train_loader))\n",
    "    print(\"Train Accuracy:\", 100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e4f8c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"final_deepfake_model.pth\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66aef586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[381  19]\n",
      " [  0 400]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " manipulated       1.00      0.95      0.98       400\n",
      "        real       0.95      1.00      0.98       400\n",
      "\n",
      "    accuracy                           0.98       800\n",
      "   macro avg       0.98      0.98      0.98       800\n",
      "weighted avg       0.98      0.98      0.98       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3db3433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea46f951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig(\"confusion_matrix.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01b371b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Recompute confusion matrix from predictions\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "classes = [\"Manipulated\", \"Real\"]\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xticks(np.arange(len(classes)), classes)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "\n",
    "# Write values inside boxes\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "356934ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig(\"confusion_matrix.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d467ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig(\"confusion_matrix.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e65ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f864fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:43<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.24918491206531013\n",
      "Train Accuracy: 89.89285714285714\n",
      "\n",
      "Epoch [2/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.10228030347531396\n",
      "Train Accuracy: 97.03571428571429\n",
      "\n",
      "Epoch [3/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06059946153878367\n",
      "Train Accuracy: 98.17857142857143\n",
      "\n",
      "Epoch [4/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.041881769672368785\n",
      "Train Accuracy: 98.82142857142857\n",
      "\n",
      "Epoch [5/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.035091691223954384\n",
      "Train Accuracy: 99.17857142857143\n",
      "\n",
      "Epoch [6/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.018670354930301464\n",
      "Train Accuracy: 99.53571428571429\n",
      "\n",
      "Epoch [7/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.04830601323163137\n",
      "Train Accuracy: 98.71428571428571\n",
      "\n",
      "Epoch [8/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.05986867135773147\n",
      "Train Accuracy: 98.21428571428571\n",
      "\n",
      "Epoch [9/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.022347724057617598\n",
      "Train Accuracy: 99.46428571428571\n",
      "\n",
      "Epoch [10/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.007375978840282187\n",
      "Train Accuracy: 99.89285714285714\n",
      "\n",
      "Epoch [11/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.029923655238567985\n",
      "Train Accuracy: 99.39285714285714\n",
      "\n",
      "Epoch [12/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.014461321548276049\n",
      "Train Accuracy: 99.5\n",
      "\n",
      "Epoch [13/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.03142180979642684\n",
      "Train Accuracy: 98.92857142857143\n",
      "\n",
      "Epoch [14/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.020999879073434775\n",
      "Train Accuracy: 99.39285714285714\n",
      "\n",
      "Epoch [15/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.024516831033189582\n",
      "Train Accuracy: 99.28571428571429\n",
      "\n",
      "Epoch [16/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.02101181459284687\n",
      "Train Accuracy: 99.35714285714286\n",
      "\n",
      "Epoch [17/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 20.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.008812312982398518\n",
      "Train Accuracy: 99.82142857142857\n",
      "\n",
      "Epoch [18/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0019108798244269565\n",
      "Train Accuracy: 100.0\n",
      "\n",
      "Epoch [19/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006572118506301194\n",
      "Train Accuracy: 100.0\n",
      "\n",
      "Epoch [20/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:16<00:00, 21.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009947870232333246\n",
      "Train Accuracy: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\"Loss:\", running_loss / len(train_loader))\n",
    "    print(\"Train Accuracy:\", 100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0812d5d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_deepfake_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"final_deepfake_model.pth\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fe4865f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m----> 9\u001b[0m         images \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     11\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
